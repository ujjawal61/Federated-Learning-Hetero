{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_train_minor = transforms.Compose([\n",
    "    transforms.RandomPerspective(),\n",
    "    transforms.ColorJitter(0.1,0.2,0.2,0.1),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#loading train dataset\n",
    "dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=transform_train)\n",
    "#loading test dataset\n",
    "testset = datasets.CIFAR10('../data/cifar-t',train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is sampling the cifar image index in non iid way, each client will have majority of 2 class and 8 minor class\n",
    "def cifar_noniid2(dataset, num_users, p):\n",
    "\n",
    "    idxs = np.arange(len(dataset),dtype=int)\n",
    "    labels = np.array(dataset.targets)\n",
    "    label_list = np.unique(dataset.targets)\n",
    "    \n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n",
    "    #print(idxs_labels)\n",
    "    idxs = idxs_labels[0,:]\n",
    "    idxs = idxs.astype(int)\n",
    "    n_data=1000\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "\n",
    "    #Sample majority class for each user\n",
    "    user_majority_labels = []\n",
    "    for i in range(num_users):\n",
    "        majority_labels = np.random.choice(label_list, 2, replace = False)\n",
    "        user_majority_labels.append(majority_labels)\n",
    "\n",
    "        #label_list = list(set(label_list) - set(majority_labels))\n",
    "       # print(i,majority_labels)\n",
    "        majority_label_idxs = (majority_labels[0] == labels[idxs]) | (majority_labels[1] == labels[idxs])        \n",
    "        \n",
    "        sub_data_idxs = np.random.choice(idxs[majority_label_idxs], int(p*n_data), replace = False)\n",
    "        \n",
    "        dict_users[i] = np.concatenate((dict_users[i],sub_data_idxs))\n",
    "        idxs = np.array(list(set(idxs) - set(sub_data_idxs)))\n",
    "    if(p < 1.0):\n",
    "        for i in range(num_users):\n",
    "            majority_labels = user_majority_labels[i]\n",
    "            \n",
    "            non_majority_label_idxs = (majority_labels[0] != labels[idxs]) & (majority_labels[1] != labels[idxs])\n",
    "            \n",
    "            sub_data_idxs = np.random.choice(idxs[non_majority_label_idxs], int(p*n_data), replace = False)\n",
    "            \n",
    "            dict_users[i] = np.concatenate((dict_users[i], sub_data_idxs))\n",
    "            idxs = np.array(list(set(idxs) - set(sub_data_idxs)))\n",
    "\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users = cifar_noniid2(dataset_train,49,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ujjaw\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\n",
      "  warnings.warn(\"Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\")\n"
     ]
    }
   ],
   "source": [
    "dataset_train=list(dataset_train)\n",
    "count=50000 #last index of cifar train img, so we have to start from that\n",
    "\n",
    "#using image augmentation twice 2 add 2 transformed image \n",
    "for j in range(5):\n",
    "    for i in dict_users[j][500:]: #starting from 500 because after 500 all the minor class image will start\n",
    "        new_image=transform_train_minor(dataset_train[i][0])\n",
    "        label=dataset_train[i][1]\n",
    "        dataset_train.append([new_image,label])   \n",
    "        count += 1\n",
    "        new_tranform_idxs=count\n",
    "        dict_users[j] = np.append(dict_users[j], new_tranform_idxs)  \n",
    "        \n",
    "for j in range(5):\n",
    "    for i in dict_users[j][500:]:\n",
    "        new_image=transform_train_minor(dataset_train[i][0])\n",
    "        label=dataset_train[i][1]\n",
    "        dataset_train.append([new_image,label])   \n",
    "        count += 1\n",
    "        new_tranform_idxs=count\n",
    "        dict_users[j] = np.append(dict_users[j], new_tranform_idxs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to check classes datapoint\n",
    "temp=list()\n",
    "for i in dict_users[0]:\n",
    "     temp.append(dataset_train[i][1])\n",
    "\n",
    "# for i in range(9):\n",
    "#     print(i,'\\t',temp.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a batch data of 2500 imgs per client\n",
    "lb=LabelBinarizer()\n",
    "def batch_data(data_shard, bs=2500):\n",
    "    label=[]\n",
    "    \n",
    "    for i in range(len(data_shard)):\n",
    "        label.append(dataset_train[i][1])\n",
    "    label=lb.fit_transform(label)\n",
    "    \n",
    "    data=[]\n",
    "    for i in range(len(data_shard)):\n",
    "        data.append(dataset_train1[i]) \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the tensor datatype to numpy datatype\n",
    "dataset_train1=[]\n",
    "for i in range(len(dataset_train)):\n",
    "    temp=dataset_train[i][0].numpy()\n",
    "    dataset_train1.append(np.rollaxis(temp,0,3))\n",
    "\n",
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in dict_users.items():\n",
    "    clients_batched[client_name] = batch_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset1=[]\n",
    "for i in range(len(testset)):\n",
    "    temp=testset[i][0].numpy()\n",
    "    testset1.append(np.rollaxis(temp,0,3))\n",
    "    \n",
    "#process and batch the test set  \n",
    "label=[]\n",
    "for i in range(2000):\n",
    "    label.append(testset[i][1])\n",
    "label=lb.fit_transform(label)\n",
    "\n",
    "data=[]\n",
    "for i in range(2000):\n",
    "    data.append(testset1[i])   \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((list(data), list(label))).batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a simple NN\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(classes):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32,32,3)))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(classes, activation='softmax'))\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining parameters\n",
    "lr = 0.001 \n",
    "comms_round = 200\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 0 | global_acc: 9.600% | global_loss: 2.327223777770996\n",
      "comm_round: 0 | global_acc: 10.100% | global_loss: 2.33072566986084\n",
      "comm_round: 1 | global_acc: 8.700% | global_loss: 2.3093979358673096\n",
      "comm_round: 1 | global_acc: 10.100% | global_loss: 2.302968740463257\n",
      "comm_round: 2 | global_acc: 12.900% | global_loss: 2.2964718341827393\n",
      "comm_round: 2 | global_acc: 13.200% | global_loss: 2.297806739807129\n",
      "comm_round: 3 | global_acc: 15.600% | global_loss: 2.2966084480285645\n",
      "comm_round: 3 | global_acc: 14.200% | global_loss: 2.296231985092163\n",
      "comm_round: 4 | global_acc: 14.200% | global_loss: 2.281966209411621\n",
      "comm_round: 4 | global_acc: 12.800% | global_loss: 2.2833471298217773\n",
      "comm_round: 5 | global_acc: 13.200% | global_loss: 2.2951810359954834\n",
      "comm_round: 5 | global_acc: 11.900% | global_loss: 2.2939014434814453\n",
      "comm_round: 6 | global_acc: 16.000% | global_loss: 2.268751382827759\n",
      "comm_round: 6 | global_acc: 14.700% | global_loss: 2.272216796875\n",
      "comm_round: 7 | global_acc: 12.500% | global_loss: 2.291691303253174\n",
      "comm_round: 7 | global_acc: 12.400% | global_loss: 2.2897863388061523\n",
      "comm_round: 8 | global_acc: 15.000% | global_loss: 2.267791271209717\n",
      "comm_round: 8 | global_acc: 14.700% | global_loss: 2.2707502841949463\n",
      "comm_round: 9 | global_acc: 16.200% | global_loss: 2.2824409008026123\n",
      "comm_round: 9 | global_acc: 16.200% | global_loss: 2.281108856201172\n",
      "comm_round: 10 | global_acc: 15.000% | global_loss: 2.2612853050231934\n",
      "comm_round: 10 | global_acc: 15.300% | global_loss: 2.266169786453247\n",
      "comm_round: 11 | global_acc: 18.600% | global_loss: 2.279676675796509\n",
      "comm_round: 11 | global_acc: 20.300% | global_loss: 2.278949022293091\n",
      "comm_round: 12 | global_acc: 15.600% | global_loss: 2.259963035583496\n",
      "comm_round: 12 | global_acc: 16.400% | global_loss: 2.262070417404175\n",
      "comm_round: 13 | global_acc: 20.100% | global_loss: 2.2682714462280273\n",
      "comm_round: 13 | global_acc: 20.500% | global_loss: 2.2646126747131348\n",
      "comm_round: 14 | global_acc: 21.200% | global_loss: 2.2454495429992676\n",
      "comm_round: 14 | global_acc: 21.000% | global_loss: 2.2489707469940186\n",
      "comm_round: 15 | global_acc: 17.800% | global_loss: 2.271791696548462\n",
      "comm_round: 15 | global_acc: 18.500% | global_loss: 2.270942449569702\n",
      "comm_round: 16 | global_acc: 18.000% | global_loss: 2.241945266723633\n",
      "comm_round: 16 | global_acc: 19.100% | global_loss: 2.245605945587158\n",
      "comm_round: 17 | global_acc: 25.100% | global_loss: 2.256945848464966\n",
      "comm_round: 17 | global_acc: 24.200% | global_loss: 2.2551956176757812\n"
     ]
    }
   ],
   "source": [
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(10)\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    #print(global_weights)\n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer='adam', \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
